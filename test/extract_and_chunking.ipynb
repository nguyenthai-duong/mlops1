{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKaOShuO4t8v"
      },
      "source": [
        "### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mnpyt_8p4t8y"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KBG0ZzQi4t8z"
      },
      "outputs": [],
      "source": [
        "def get_text(input_path):\n",
        "    doc = fitz.open(input_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MuK8OGTv4t8z"
      },
      "outputs": [],
      "source": [
        "def remove_space_redundant(text):\n",
        "    words = text.split()\n",
        "    clean_text = \" \".join(words)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VBujbRqD4t8z"
      },
      "outputs": [],
      "source": [
        "def create_content_json(input_path, output_path):\n",
        "    text = get_text(input_path)\n",
        "\n",
        "    content_between_chapters = re.findall(r\"(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.?)(.*?)(?=(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.? |$))\", text, re.DOTALL)\n",
        "    chapter_name = []\n",
        "    content_chapter = []\n",
        "    all_content_chapter = []    # extract từng chapter trước\n",
        "    for content_between_chapter in content_between_chapters:\n",
        "        chapter_name_temp = content_between_chapter[0].strip()\n",
        "        content_chapter_temp = content_between_chapter[1].strip()\n",
        "        chapter_name.append(chapter_name_temp.strip())\n",
        "        content_chapter.append(content_chapter_temp.strip())\n",
        "        all_content_chapter.append(content_between_chapter[0] + content_between_chapter[1])\n",
        "\n",
        "    chapter_title = []\n",
        "    rule_title = []\n",
        "    contents = []\n",
        "    regex_chapter = re.compile(r'(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.?)\\s*(.*)')\n",
        "    regex_rule = re.compile(r'(Điều \\d+\\.)(.*?)(?=(Điều \\d+\\. |$))', re.DOTALL)\n",
        "    for content_chap in all_content_chapter:\n",
        "        matches_chapter = regex_chapter.findall(content_chap)\n",
        "        matches_rule = regex_rule.findall(content_chap)\n",
        "        for match_rule in matches_rule:\n",
        "            for match_chapter in matches_chapter:\n",
        "                temp = match_chapter[0] + \"\\n\" + match_chapter[1]\n",
        "                chapter_title.append(temp.strip())\n",
        "            temp_title_rule = match_rule[0]\n",
        "            rule_title.append(temp_title_rule.strip())\n",
        "            temp_content_rule = match_rule[1].strip()\n",
        "            contents.append(temp_content_rule)\n",
        "\n",
        "    titles = []\n",
        "    for i in range(len(chapter_title)):\n",
        "        titles.append(chapter_title[i] + \",\\n\" + rule_title[i])\n",
        "\n",
        "    data = []\n",
        "    for i in range(len(titles)):\n",
        "        data.append({'title': titles[i], 'content': contents[i]})\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iqYF7En4t8z"
      },
      "source": [
        "### Create content json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cWx2nv824t80"
      },
      "outputs": [],
      "source": [
        "input_path = \"luật_đất_đai_2024.pdf\"\n",
        "output_path = \"luật_đất_đai_2024.json\"\n",
        "create_content_json(input_path, output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn7ZXpa_4t80"
      },
      "source": [
        "# Create chunk json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4_UmGJtG4t80"
      },
      "outputs": [],
      "source": [
        "def create_chunk_json(input_path, output_path):\n",
        "    text = get_text(input_path)\n",
        "\n",
        "    content_between_chapters = re.findall(r\"(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.?)(.*?)(?=(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.? |$))\", text, re.DOTALL)\n",
        "    chapter_name = []\n",
        "    content_chapter = []\n",
        "    all_content_chapter = []    # extract từng chapter trước\n",
        "    for content_between_chapter in content_between_chapters:\n",
        "        chapter_name_temp = content_between_chapter[0].strip()\n",
        "        content_chapter_temp = content_between_chapter[1].strip()\n",
        "        chapter_name.append(chapter_name_temp.strip())\n",
        "        content_chapter.append(content_chapter_temp.strip())\n",
        "        all_content_chapter.append(content_between_chapter[0] + content_between_chapter[1])\n",
        "\n",
        "    chapter_title = []\n",
        "    rule_title = []\n",
        "    contents = []\n",
        "    regex_chapter = re.compile(r'(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.?)\\s*(.*)')\n",
        "    regex_rule = re.compile(r'(Điều \\d+\\.)(.*?)(?=(Điều \\d+\\. |$))', re.DOTALL)\n",
        "    for content_chap in all_content_chapter:\n",
        "        matches_chapter = regex_chapter.findall(content_chap)\n",
        "        matches_rule = regex_rule.findall(content_chap)\n",
        "        for match_rule in matches_rule:\n",
        "            for match_chapter in matches_chapter:\n",
        "                temp = match_chapter[0] + \"\\n\" + match_chapter[1]\n",
        "                chapter_title.append(temp.strip())\n",
        "            temp_title_rule = match_rule[0] + match_rule[1].split('\\n')[0].strip()\n",
        "            rule_title.append(temp_title_rule.strip())\n",
        "            temp_content_rule = remove_space_redundant(\" \".join(match_rule[1].split('\\n')[1:]).strip())\n",
        "            contents.append(temp_content_rule)\n",
        "\n",
        "    titles = []\n",
        "    for i in range(len(chapter_title)):\n",
        "        titles.append(os.path.basename(input_path).rstrip(\".pdf\").replace('_', ' ') + \"\\n\" + chapter_title[i] + \"\\n\" + rule_title[i])\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=512,\n",
        "        chunk_overlap=64)\n",
        "\n",
        "    title_chunk, chunks = [], []\n",
        "    for i in range(len(contents)):\n",
        "        chunk = text_splitter.split_text(contents[i])\n",
        "\n",
        "        num = len(chunk)\n",
        "        for k in range(num):\n",
        "            title_chunk.append(titles[i])\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    chunks = [item for chunk in chunks for item in chunk]\n",
        "\n",
        "    data = []\n",
        "    for i in range(len(title_chunk)):\n",
        "        data.append({'title': title_chunk[i], 'context': chunks[i]})\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0DXr1cbR4t80"
      },
      "outputs": [],
      "source": [
        "input_path = \"luật_đất_đai_2024.pdf\"\n",
        "output_path = \"luật_đất_đai_2024.json\"\n",
        "create_chunk_json(input_path, output_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
